# Spark Configuration File
# This file contains default Spark configuration for production use

# Application Properties
spark.app.name=SparkApp
spark.driver.memory=1g
spark.executor.memory=2g
spark.executor.cores=2

# Spark SQL
spark.sql.warehouse.dir=/opt/spark/warehouse
spark.sql.extensions=io.delta.sql.DeltaSparkSessionExtension
spark.sql.catalog.spark_catalog=org.apache.spark.sql.delta.catalog.DeltaCatalog

# Delta Lake Configuration
spark.databricks.delta.retentionDurationCheck.enabled=false
spark.databricks.delta.schema.autoMerge.enabled=true
spark.databricks.delta.properties.defaults.enableChangeDataFeed=true

# S3A Configuration (MinIO)
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.connection.ssl.enabled=false
spark.hadoop.fs.s3a.attempts.maximum=3
spark.hadoop.fs.s3a.connection.timeout=10000
spark.hadoop.fs.s3a.fast.upload=true
spark.hadoop.fs.s3a.multipart.size=104857600
spark.hadoop.fs.s3a.buffer.dir=/tmp/s3a

# Performance Tuning
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrationRequired=false
spark.shuffle.compress=true
spark.shuffle.spill.compress=true
spark.io.compression.codec=snappy

# Dynamic Allocation (disabled for static cluster)
spark.dynamicAllocation.enabled=false

# Event Log
spark.eventLog.enabled=true
spark.eventLog.dir=/opt/spark/logs
spark.history.fs.logDirectory=/opt/spark/logs

# Network
spark.network.timeout=300s
spark.executor.heartbeatInterval=60s

# UI
spark.ui.reverseProxy=false
spark.ui.reverseProxyUrl=

# Security (disable for development, enable in production)
spark.authenticate=false
spark.network.crypto.enabled=false
spark.io.encryption.enabled=false

# Metrics
spark.metrics.conf.*.sink.console.class=org.apache.spark.metrics.sink.ConsoleSink
spark.metrics.conf.*.sink.console.period=10
spark.metrics.conf.*.sink.console.unit=seconds

spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version=2
spark.hadoop.fs.s3a.committer.name=partitioned # or directory
spark.hadoop.fs.s3a.committer.staging.conflict-mode=append
spark.hadoop.fs.s3a.committer.staging.tmp.path=/tmp/s3a-staging